{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML Example\n",
    "\n",
    "This code block is to import:\n",
    "* FindSpark\n",
    "* SparkSession\n",
    "* Spark SQL functions\n",
    "\n",
    "And then initialises the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"FSTExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.22631.4169]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "(.venv) c:\\Users\\sssah\\Desktop\\FSTM2\\BigData\\Spark>C:/Users/sssah/Desktop/FSTM2/BigData/.venv/Lib/site-packages/pyspark/bin/spark-shell --packages com.databricks:spark-xml_2.12:0.18.0\n",
      ":: loading settings :: url = jar:file:/C:/Users/sssah/Desktop/FSTM2/BigData/Spark/.venv/Lib/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: C:\\Users\\sssah\\.ivy2\\cache\n",
      "The jars for the packages stored in: C:\\Users\\sssah\\.ivy2\\jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-afeed993-0410-495e-814a-20c354e729f2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.18.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in local-m2-cache\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.18.0/spark-xml_2.12-0.18.0.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#spark-xml_2.12;0.18.0!spark-xml_2.12.jar (705ms)\n",
      "downloading file:/C:/Users/sssah/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.11.0!commons-io.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/jaxb/txw2/3.0.2/txw2-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish.jaxb#txw2;3.0.2!txw2.jar (429ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/ws/xmlschema/xmlschema-core/2.3.0/xmlschema-core-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.ws.xmlschema#xmlschema-core;2.3.0!xmlschema-core.jar(bundle) (486ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.9.0/scala-collection-compat_2.12-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.9.0!scala-collection-compat_2.12.jar (650ms)\n",
      ":: resolution report :: resolve 23995ms :: artifacts dl 2316ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.18.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from local-m2-cache in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-afeed993-0410-495e-814a-20c354e729f2\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (989kB/168ms)\n",
      "24/10/03 12:12:46 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "24/10/03 12:12:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/03 12:13:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/10/03 12:13:55 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/10/03 12:13:55 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/10/03 12:13:57 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\n",
      "\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\n",
      "\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$14(Executor.scala:1163)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$14$adapted(Executor.scala:1155)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1155)\n",
      "\tat org.apache.spark.executor.Executor.<init>(Executor.scala:330)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2883)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n",
      "\tat org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)\n",
      "\tat $line3.$read$$iw$$iw.<init>(<console>:15)\n",
      "\tat $line3.$read$$iw.<init>(<console>:42)\n",
      "\tat $line3.$read.<init>(<console>:44)\n",
      "\tat $line3.$read$.<init>(<console>:48)\n",
      "\tat $line3.$read$.<clinit>(<console>)\n",
      "\tat $line3.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line3.$eval$.$print(<console>:6)\n",
      "\tat $line3.$eval.$print(<console>)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$quietRun$1(IMain.scala:216)\n",
      "\tat scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:206)\n",
      "\tat scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:216)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$2(SparkILoop.scala:83)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$1(SparkILoop.scala:83)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:97)\n",
      "\tat org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:83)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$process$4(SparkILoop.scala:165)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.$anonfun$mumly$1(ILoop.scala:166)\n",
      "\tat scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:206)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:163)\n",
      "\tat org.apache.spark.repl.SparkILoop.loopPostInit$1(SparkILoop.scala:153)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:221)\n",
      "\tat org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189)\n",
      "\tat org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201)\n",
      "\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236)\n",
      "\tat org.apache.spark.repl.Main$.doMain(Main.scala:78)\n",
      "\tat org.apache.spark.repl.Main$.main(Main.scala:58)\n",
      "\tat org.apache.spark.repl.Main.main(Main.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "\t... 6 more\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "24/10/03 12:13:58 ERROR Utils: Uncaught exception in thread main\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.rpc.RpcEndpointRef.ask(Object, scala.reflect.ClassTag)\" because the return value of \"org.apache.spark.scheduler.local.LocalSchedulerBackend.localEndpoint()\" is null\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.org$apache$spark$scheduler$local$LocalSchedulerBackend$$stop(LocalSchedulerBackend.scala:173)\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:144)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:103)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:706)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2883)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n",
      "\tat org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)\n",
      "\tat $line3.$read$$iw$$iw.<init>(<console>:15)\n",
      "\tat $line3.$read$$iw.<init>(<console>:42)\n",
      "\tat $line3.$read.<init>(<console>:44)\n",
      "\tat $line3.$read$.<init>(<console>:48)\n",
      "\tat $line3.$read$.<clinit>(<console>)\n",
      "\tat $line3.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line3.$eval$.$print(<console>:6)\n",
      "\tat $line3.$eval.$print(<console>)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$quietRun$1(IMain.scala:216)\n",
      "\tat scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:206)\n",
      "\tat scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:216)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$2(SparkILoop.scala:83)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$1(SparkILoop.scala:83)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:97)\n",
      "\tat org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:83)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$process$4(SparkILoop.scala:165)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.$anonfun$mumly$1(ILoop.scala:166)\n",
      "\tat scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:206)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:163)\n",
      "\tat org.apache.spark.repl.SparkILoop.loopPostInit$1(SparkILoop.scala:153)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:221)\n",
      "\tat org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189)\n",
      "\tat org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201)\n",
      "\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236)\n",
      "\tat org.apache.spark.repl.Main$.doMain(Main.scala:78)\n",
      "\tat org.apache.spark.repl.Main$.main(Main.scala:58)\n",
      "\tat org.apache.spark.repl.Main.main(Main.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "24/10/03 12:13:58 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n",
      "24/10/03 12:13:59 ERROR Main: Failed to initialize Spark session.\n",
      "java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\n",
      "\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\n",
      "\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\n",
      "\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\n",
      "\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$14(Executor.scala:1163)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$14$adapted(Executor.scala:1155)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n",
      "\tat org.apache.spark.executor.Executor.updateDependencies(Executor.scala:1155)\n",
      "\tat org.apache.spark.executor.Executor.<init>(Executor.scala:330)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2883)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n",
      "\tat org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)\n",
      "\tat $line3.$read$$iw$$iw.<init>(<console>:15)\n",
      "\tat $line3.$read$$iw.<init>(<console>:42)\n",
      "\tat $line3.$read.<init>(<console>:44)\n",
      "\tat $line3.$read$.<init>(<console>:48)\n",
      "\tat $line3.$read$.<clinit>(<console>)\n",
      "\tat $line3.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line3.$eval$.$print(<console>:6)\n",
      "\tat $line3.$eval.$print(<console>)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
      "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
      "\tat scala.tools.nsc.interpreter.IMain.$anonfun$quietRun$1(IMain.scala:216)\n",
      "\tat scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:206)\n",
      "\tat scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:216)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$2(SparkILoop.scala:83)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$1(SparkILoop.scala:83)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:97)\n",
      "\tat org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:83)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$process$4(SparkILoop.scala:165)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.$anonfun$mumly$1(ILoop.scala:166)\n",
      "\tat scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:206)\n",
      "\tat scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:163)\n",
      "\tat org.apache.spark.repl.SparkILoop.loopPostInit$1(SparkILoop.scala:153)\n",
      "\tat org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:221)\n",
      "\tat org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189)\n",
      "\tat org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201)\n",
      "\tat org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236)\n",
      "\tat org.apache.spark.repl.Main$.doMain(Main.scala:78)\n",
      "\tat org.apache.spark.repl.Main$.main(Main.scala:58)\n",
      "\tat org.apache.spark.repl.Main.main(Main.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\n",
      "\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\n",
      "\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\n",
      "\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\n",
      "\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "\t... 6 more\n",
      "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\n",
      "\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\n",
      "\t... 25 more\n",
      "24/10/03 12:13:59 ERROR Utils: Uncaught exception in thread shutdown-hook-0\n",
      "java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.executor.Executor.stop(Executor.scala:429)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$stopHookReference$1(Executor.scala:90)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<init>(ShuffleBlockPusher.scala:499)\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<clinit>(ShuffleBlockPusher.scala)\n",
      "\t... 16 more\n",
      "24/10/03 12:13:59 WARN ShutdownHookManager: ShutdownHook '' failed, java.util.concurrent.ExecutionException: java.lang.ExceptionInInitializerError\n",
      "java.util.concurrent.ExecutionException: java.lang.ExceptionInInitializerError\n",
      "\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.executeShutdown(ShutdownHookManager.java:124)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:95)\n",
      "Caused by: java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.executor.Executor.stop(Executor.scala:429)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$stopHookReference$1(Executor.scala:90)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.conf()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<init>(ShuffleBlockPusher.scala:499)\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<clinit>(ShuffleBlockPusher.scala)\n",
      "\t... 16 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(.venv) c:\\Users\\sssah\\Desktop\\FSTM2\\BigData\\Spark>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "C:/Users/sssah/Desktop/FSTM2/BigData/.venv/Lib/site-packages/pyspark/bin/spark-shell --packages com.databricks:spark-xml_2.12:0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o28.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.datbricks.spark.xml. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.ClassNotFoundException: com.datbricks.spark.xml.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcom.datbricks.spark.xml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrowTag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./sample_data/persons.xml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\sssah\\Desktop\\FSTM2\\BigData\\Spark\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sssah\\Desktop\\FSTM2\\BigData\\Spark\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\sssah\\Desktop\\FSTM2\\BigData\\Spark\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\sssah\\Desktop\\FSTM2\\BigData\\Spark\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.datbricks.spark.xml. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.ClassNotFoundException: com.datbricks.spark.xml.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.datbricks.spark.xml').option('rowTag','person').load('./sample_data/persons.xml')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
