# FST-M2

The following is a checklist of activities that are to be uploaded to the repo:

**JMeter**:
- [ ] Activity 1(JMX File) - To run a basic load test to demonstarate the basic elements in JMeter - Thread Group, Sampler, Assertions, Listener, Timers
- [ ] Activity 2(JMX File) - To do a basic load test on a website using Basic Authentication
- [ ] Activity 3(JMX File) - To implement basic load test using pre and post processors on the training support Selenium site
- [ ] Actvitiy 4(JMX File) - Connecting to a database and run a load test
- [ ] Actvitiy 5(JMX File) - Loading data from a CSV file and using that data in a HTTP Request
- [ ] Activity 6(JMX File) - A demonstration of how the HTTP Link parser works
- [ ] Activity 7(JMX File) - A Spidering example using the Selenium website
- [ ] Activity 8(JMX File) - To run a basic load test on the Selenium site for Training Support

**JMeter Project**:
- [ ] Activity 1(JMX File) - Recording a load test
- [ ] Activity 2(JMX File) - Load testing an API with Authentication
- [ ] Activity 3(JMX File) - Petstore API

**Linux**:
- [ ] Activity 1(Screenshot) - Commands to show how to interact with the filesystem using basic shell commands.
- [ ] Activity 2(Screenshot) - Show how to use the vim text editor to edit text at the terminal.
- [ ] Activity 3(Screenshot) - Commands to show how to redirect the flow of data between STDIN/STDOUT/STDERR
- [ ] Activity 4(.sh file) - Create a shell script that creates 6 file each of types .png, .mp3, and .mp4. Move these to Image, Songs, and Movies folders respectively.

**Docker**:
- [ ] Activity 1(Screenshot) - Command to create a container with the hello-world image
- [ ] Activity 2(Screenshot) - Run a default commands from the CLI as soon as the container starts
- [ ] Activity 3(Screenshot) - Various Podman CLI commands(start, stop, ps, rm, etc.)
- [ ] Activity 4(Screenshot) - Pushing image to remote registries
- [ ] Activity 5(Dockerfile) - Create and run a container that has a NodeJS app inside it
- [ ] Activity 6(Dockerfile, docker-compose.yml) - Create and run a NodeJS app along with a DB container using podman networks
- [ ] Activity 7(Dockerfile, docker-compose.yml) - Create and run a NodeJS app along with a persistently stored DB container using podman compose
- [ ] Activity 8(Dockerfile, docker-compose.yml) - Create and run a complex application that has multiple services


**Kubernetes**:
- [ ] Activity 1(YML files) - Create a configuration file to setup and run a simple Kubernetes cluster
- [ ] Activity 2(Screenshot) - Commands to run and show kubectl run, describe, and delete
- [ ] Activity 3(YML files) - Create a configuration file to setup and run an application with multiple services running in a cluster
- [ ] Activity 4, 5(YML files) - Create a configuration file to setup and run a replicaset for an nginx pod. Using the previous nginx replicaset, update the image in the pods, and then roll it back
- [ ] Activity 6(Screenshot) - Kubernetes imperative deployment commands
- [ ] Activity 7(Screenshot) - Create a configuration file to setup and run an application with multiple services and persistent storage running in a cluster

**Docker Project**:
- [ ] The following files have to be uploaded - Dockerfile.dev, Dockerfile.prod, docker-compose.yml, projectActivity.jmx

**Kubernetes Project**:
- [ ] The following files have to be uploaded - api-deployment.yml, api-cluster-ip-service.yml, postgres-deployment.yml, postgres-cluster-ip-service.yml, ingress-service.yml, database-persistent-volume-claim.yml

**OpenShift**:
- [ ] Activity 1(Screenshot) - Log into a running OpenShift Cluster
- [ ] Activity 2(Screenshot) - Copy file to and from a running container without rebuilding the container image
- [ ] Activity 3(Screenshot) - Using the oc cli tool, enumerate, describe, and update application resource objects on OpenShift.
- [ ] Activity 4(Screenshot) - Run a database with persistent storage on OpenShift
- [ ] Activity 5(Screenshot) - Deploy an application from its source code onto OpenShift using Source-to-Image
- [ ] Activity 6(Screenshot) - Deploy a pre-built application on OpenShift with the web console and the oc command line tool
- [ ] Activity 7(Screenshot) - Scale an application using the Web Console and the CLI options

**Ansible**:
- [ ] Activity 1(YML file) - Write a simple Ansible playbook
- [ ] Activity 2(YML file) - Write an Ansible playbook to configure an application server

**Maven**:
- [ ] Activity 1(Screenshot) - Running the various lifecycle commands
- [ ] Activity 2(pom.xml) - Configure a pom.xml to use a plugin and observe how it effects the execution during build

**Hadoop**:
- [ ] Activity 1(Screenshot) - To manipulate files in the Hadoop Distributed File System(HDFS)
- [ ] Activity 2(PIG file) - MapReduce using Pig on Text File
- [ ] Activity 3(PIG file) - MapReduce using Pig on CSV File
- [ ] Activity 4(Screenshot) - MapReduce using Pig on Text File with localmode option
- [ ] Activity 5(Screenshot) - MapReduce using Pig on CSV File with localmode option
- [ ] Activity 6(HIVE file) - WordCount using Hive on Text file
- [ ] Activity 7(HIVE file) - Analyzing employee data using Hive. Writing queries in Hive to create a table and insert data and perform various MapReduce operations.
- [ ] Activity 8(Screenshot) - Establish a connection using it and how to create databases and tables using Beeline
- [ ] Activity 9(HIVE file) - Using Beeline, participants will connect to a database and load data from CSV files and perform various MapReduce operations

**Spark:**
- [ ] Activity 1(Notebook file) - Using Notebooks to run Spark computations text files
- [ ] Activity 2(Notebook file) - Using Notebooks to run Spark computations on CSV Files
- [ ] Activity 3(Notebook file) - Using Notebooks to run Spark computations on JSON Files
- [ ] Activity 4(Notebook file) - Using Notebooks to run Spark computations on XML Files
- [ ] Activity 5(Notebook file) - Using Notebooks to run Spark while connecting to a Hive database

**Informatica**:
- [ ] Activity 1(Screenshots) - Using Informatica to perform a simple Filter Mapping
- [ ] Activity 2(Screenshots) - Using Informatica to perform Mapping a complex Filter Mapping with the results bring routed through other transformations
- [ ] Activity 3(Screenshots) - Using Informatica to perform Mapping with the Lookup Transformation
- [ ] Activity 4(Screenshots) - Using Informatica to perform a Mapping with the Joiner Transfomation
- [ ] Activity 5(Screenshots) - Using Informatica to perform a Mapping that uses a database View

**Hadoop Project**:
- [ ] Activity 1(PIG file) - Count the number of lines spoken by each character
- [ ] Activity 2(HIVE file) - Count the number of lines spoken by each character
- [ ] Activity 3(HIVE file) - Count the number of dialogues where the name "Luke" is said in EpisodeIV

**Informatica Project**:
- [ ] Activity 1(Screenshot) - Create a Mapping in Informatica Cloud to load the maximum price amongst all the cars
- [ ] Activity 2(Screenshot) - Create a Mapping in Informatica Cloud to load the minimum price amongst all the cars

**QuerySurge**:
- [ ] Activity 1(Screenshot) - Creating QueryPairs using the Query Wizard
- [ ] Activity 2(Screenshot) - Creating a TestSuite and executing it
- [ ] Activity 3(Screenshot) - Creating a Table-level comparison using the Query Wizard
